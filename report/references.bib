@comment{x-kbibtex-encoding=utf-8}

@comment{jabref-meta: databaseType:bibtex;}

@article{McCulloch1943,
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	author = {McCulloch, Warren S. and Pitts, Walter},
	doi = {10.1007/BF02478259},
	issn = {1522-9602},
	journal = {The bulletin of mathematical biophysics},
	number = {4},
	pages = {115–133},
	risfield_0_da = {1943/12/01},
	title = {A logical calculus of the ideas immanent in nervous activity},
	url = {https://link.springer.com/article/10.1007/BF02478259},
	urldate = {2021-04-06},
	volume = {5},
	year = {1943}
}

@article{Rumelhart1986,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	doi = {10.1038/323533a0},
	issn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533–536},
	risfield_0_da = {1986/10/01},
	title = {Learning representations by back-propagating errors},
	url = {https://www.nature.com/articles/323533a0},
	urldate = {2021-04-06},
	volume = {323},
	year = {1986}
}

@misc{matlab-dataset,
	author = {MathWorks},
	title = {Sample Data Sets for Shallow Neural Networks},
	url = {https://es.mathworks.com/help/deeplearning/gs/sample-data-sets-for-shallow-neural-networks.html},
	urldate = {2021-04-06}
}

@article{alexnet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	journaltitle = {Advances in neural information processing systems},
	pages = {1097–1105},
	title = {ImageNet Classification with Deep Convolutional Networks},
	url = {https://kr.nvidia.com/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf},
	urldate = {2021-04-05},
	volume = {25},
	year = {2012}
}

@misc{tf.keras.optimizers.SGD,
	author = {TensorFlow},
	title = {tf.keras.optimizers.SGD - Documentation},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD},
	urldate = {2021-05-02},
	year = {2019}
}

@misc{paperspace-mom-rmsprop-adam,
	author = {Kathuria, Ayoosh},
	title = {Intro to optimization in deep learning: Momentum, RMSProp and Adam},
	url = {https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam},
	urldate = {2021-05-02},
	year = {2018}
}

@misc{understanding-rmsprop,
	author = {Bushaev, Vitaly},
	month = sep,
	organization = {Towards Data Science},
	title = {Understanding RMSprop — faster neural network learning},
	url = {https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a},
	urldate = {2021-05-02},
	year = {2018}
}

@misc{rmsprop-hinton,
	author = {Tieleman, T. and Hinton, G.},
	howpublished = {COURSERA: Neural Networks for Machine Learning},
	title = {Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	urldate = {2021-05-02},
	year = {2012}
}

@article{adam-paper,
	author = {Kingma, Diederik and Ba, Jimmy},
	journal = {International Conference on Learning Representations},
	month = {12},
	title = {Adam: A Method for Stochastic Optimization},
	url = {https://www.researchgate.net/publication/269935079_Adam_A_Method_for_Stochastic_Optimization},
	urldate = {2021-05-02},
	year = {2014}
}

@misc{tds-adam,
	author = {Bushaev, Vitaly},
	month = oct,
	organization = {Towards Data Science},
	title = {Adam — latest trends in deep learning optimization.},
	url = {https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c},
	urldate = {2021-05-02},
	year = {2018}
}

